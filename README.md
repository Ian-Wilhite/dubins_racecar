# Dubins Racecar RL vs OC

This project compares **Reinforcement Learning (RL)** with **Optimal Control (OC)** for a racecar navigating closed tracks using Dubins path planning. The initial environment models a standard 400m stadium track, and the goal is to complete a full lap.

---

## ğŸ§  Project Goals

* **Evaluate** path-following performance of RL agents vs. optimal control.
* **Develop** a modular simulation framework with swappable environments and controllers.
* **Benchmark** trajectories, lap times, and deviations from the ideal path.

---

## ğŸ“ Code Structure

```
dubins_racecar/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ optimal_control_agent.py    # OC agent using Dubins paths
â”œâ”€â”€ environments/
â”‚   â””â”€â”€ stadium_track.py           # 400m oval stadium Gym environment
â”œâ”€â”€ planners/
â”‚   â””â”€â”€ dubins_planner.py          # Dubins path generator
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ geometry.py                # Math helpers
â”‚   â”œâ”€â”€ metrics.py                 # Evaluation metrics
â”‚   â””â”€â”€ visualization.py           # Plotting utilities
â”œâ”€â”€ train_and_test_rl.py          # Stable Baselines3 RL training + testing
â””â”€â”€ README.md                      # Project overview and instructions
```

---

## ğŸš€ How to Run

### 1. Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Required packages:

* `stable-baselines3`
* `gymnasium`
* `matplotlib`
* `shapely`
* `dubins`

You can install them manually:

```bash
pip install stable-baselines3 gymnasium matplotlib shapely dubins
```

### 2. Train the RL Agent

```bash
python dubins_racecar/train_and_test_rl.py
```

This will:

* Create the stadium track environment
* Train a PPO agent for 50,000 steps
* Test and visualize the final trajectory

### 3. Run Optimal Control Agent (TBD)

Create a separate script to initialize and evaluate `OptimalControlAgent` from `agents/`.

---

## ğŸ“Š Evaluation Metrics (Planned)

* Lap time (seconds)
* Deviation from track centerline
* Reward accumulation (RL only)
* Success rate across different environments

---

## ğŸ”§ Extending the Framework

You can:

* Add new environments to `environments/`
* Swap planners or control policies
* Benchmark more advanced RL agents (e.g., SAC, TD3)
